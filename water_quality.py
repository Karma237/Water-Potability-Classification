# -*- coding: utf-8 -*-
"""Water Quality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rqBrJXSKjfJRTcy99xFIyrJRZ_7xsxvi

# My Libraries & Packages Installation
"""

import pandas as pd
import numpy as np
import copy
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

"""Load and Read data"""

from google.colab import files
uploaded = files.upload()

original_data = pd.read_csv('water_potability.csv')
data = copy.deepcopy(original_data)
data.head()

"""## Data Analysis

*Knowing my data*
"""

print('\nData Information:\n')
print(data.info())

print('\nData Shape:\n' , data.shape)

print('\nFind Duplicates:\n', data.duplicated().sum())

tMissing = int((data.isnull().sum().sum() / np.product(data.shape)) * 100)
print("\nTotal Missing values from data is \n" , tMissing, "%")

NullCol = data[data.columns[data.isnull().any()]]
print('\nColumns Containing Null values:\n', (NullCol.isnull().sum()))

multi_null_rows = data.isnull().sum(axis=1) > 1
print('\nNumber of rows with more than one null value:', multi_null_rows.sum())

"""---



> The training-set has 3276 examples and 9 features + the target variable (potability). 9 of the features are floats and 1 is integer.

  

> Scientific search info;





  1. PH:-  Measure of the acidity or alkalinity of water. It is a logarithmic scale that ranges from 0 to 14, with 7 being neutral.

  2. Hardness:-  The concentration of calcium and magnesium ions in water.

  3. Solids:-  The total amount of dissolved and suspended matter in water.

  4. Chloramines:-   A disinfectant used to treat water, formed by combining chlorine and ammonia.

  5. Sulfate:- The concentration of sulfate ions in water.

  6. Conductivity:-  A measure of the water's ability to conduct electricity, indicate the presence of dissolved minerals or contaminants.

  7. Organic carbon:-  The amount of organic matter present in water, which can include natural substances and pollutants.

  8. Trihalomethanes:-   A group of DBPs formed when chlorine or other disinfectants react with organic matter in water.

  9. Turbidity:-  A measure of the water's clarity, caused by suspended particles,  indicate the presence of contaminants.



---

*Statistical Analysis*
"""

data.describe().T

"""*Outliers*"""

def detect_outliers_iqr(data):
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    outliers = data[(data < lower_bound) | (data > upper_bound)].index.tolist()
    return outliers

for column in data.columns:
  outliers = detect_outliers_iqr(data[column])
  num_outliers = len(outliers)
  print(f"{column}: {num_outliers} outliers")

"""*Distribution*"""

fig = plt.figure(figsize = (10,10))
var = data.drop("Potability", axis=1)
ax = fig.gca()
var.hist(ax=ax)
plt.show();

"""---




> The features exhibit a similar distribution and resemble a Gaussian shape, indicating that normalization may not be necessary.



---

*Correlation between Features*
1. Quantifies the strength and direction of the linear relationship between two variables.
2. Used for feature engineering, selection, data exploration, and model evaluation.
3. Helps identify redundant features and select relevant features.
4. Provides insights into the relationships between variables in your dataset.
5. Does not imply causation.
6. Should be used in conjunction with other techniques and domain knowledge..
"""

plt.figure(figsize=(10,10))
sns.heatmap(data.corr(),annot=True,fmt=".2g")
plt.show()

"""---


> Potability is showing 0.034 (highest positive value) with relation to Solids.




---

*Linearity*
"""

sns.pairplot(data, x_vars=data.columns[:-1], y_vars='Potability')
plt.show()

"""---

> Indicating Non linearity.
Thus, Ignoring Linear models as
1. Logistic regression
2. SVM
3. Naive bias

Suggestioning useage of ither Tree based models as
1. Decision tree
2. Random Forest
3. XGBoost (if needed)


---

Studing Target Variable
"""

plt.clf()
plt.style.use('ggplot')
fig1, ax1 = plt.subplots()
ax1.pie(data['Potability'].value_counts(), colors=['cyan', 'green'], labels=['Non Potable', 'Potable'],
        autopct='%1.1f%%', startangle=0, rotatelabels=False)
centre_circle = plt.Circle((0, 0), 0.80, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)
ax1.axis('equal')
plt.tight_layout()
plt.show()

"""---


> Data is Balanced



---

*Feature Importance & Selection*
1. Improved performance: Reduces noise, improves accuracy, prevents overfitting.
2. Enhanced interpretability: Understands model's workings, predicts reasons.
3. Computational efficiency: Reduces training and deployment costs.
4. Data privacy: Protects sensitive information.
5. Improved efficiency: Streamlines data analysis and modeling.
"""

x, y = data.drop('Potability', axis=1), data['Potability']

classification = RandomForestClassifier()
classification.fit(x, y)

variables = x.columns
importance = pd.DataFrame()

importance['Features'] = variables
importance['Importance'] = classification.feature_importances_
importance = importance.sort_values(by=['Importance'], ascending=True)

labels = importance['Features']
values = importance['Importance']

# Create a  color palette
colors = ['#0000FF', '#66B2FF', '#FFD700', '#FF8C00', '#008000', '#90EE90', '#D3D3D3', '#B0C4DE', '#FF6600']

plt.figure(figsize=(10, 10))
plt.pie(values, labels=labels, autopct='%0.1f%%', startangle=90, colors=colors)
plt.title("Feature Importance - Random Forest Classifier", fontsize=16)
plt.axis('equal')  # Equal aspect ratio ensures a circular pie chart
plt.show()

"""---


> The pie chart shows that the features are relatively similar in importance, making it difficult to justify eliminating any one without also removing others.


---

## Data Cleaning & Preprocessing

Handling Missing Values

Handling duplicates; In our case no duplicates
"""

# Drop rows with more than one null
def drop_rows_with_more_than_one_null(data):
    data.dropna (thresh=len(data.columns), inplace=True)
    return data

def drop_NA_rows(data):
    data = data.dropna()
    return data

def fill_NA_with_mean(data):
    data = data.fillna(data.mean())
    return data

def hot_deck_imputation(data):
    hot_deck_imputer = KNNImputer(n_neighbors=2, weights="uniform")
    data = hot_deck_imputer.fit_transform(data)
    data = pd.DataFrame(data, columns=['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate',
                                       'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity', 'Potability'])
    return data

"""Handling Outliers"""

# First, find boundary values:
HA = data.mean() + 3 * data.std()
LA = data.mean() - 3 * data.std()


# 2.1. Trimming of Outliers
def trim_outliers(data):
    data = data[(data < HA) & (data > LA)]
    return data

def cap_outliers(data):
    data = np.where(
        data > HA,
        HA,
        np.where(
            data < LA,
            LA,
            data
        )
    )
    data = pd.DataFrame(data, columns=['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate',
                                       'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity', 'Potability'])
    return data

"""Sampling"""

def random_sampling(data):
    data_sample = data.sample(frac=0.8, replace=True, random_state=1)
    return data_sample

def stratified_sampling(data):
    data_groups = data.groupby('Potability', group_keys=False)
    data_samples = data_groups.apply(lambda x: x.sample(frac=0.6))
    return data_samples

"""*Preprocessing Pipeline*"""

data = copy.deepcopy(original_data)

data = drop_rows_with_more_than_one_null(data)
data = fill_NA_with_mean(data)
data = hot_deck_imputation(data)
data = trim_outliers(data)
data = random_sampling(data)
x, y = data.drop('Potability', axis=1), data['Potability']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""##  Classifiers"""

results = [] # For Visualizing
names = ["KNN",  "Decision Tree",  "Random Forest"]

"""*Evaluation Matrics*
1. Accuracy: Overall correct predictions.
2. Mean squared error (MSE): Average squared difference between predicted and actual values.
3. Precision: Proportion of correct positive predictions out of all positive predictions.
4. Recall: Proportion of correct positive predictions out of all actual positive instances.
5. Confusion matrix: Table showing true positives, true negatives, false positives, and false negatives.
"""

# KNN

neighbors = KNeighborsClassifier(n_neighbors=2)
neighbors.fit(x_train, y_train)
y_predKNN = neighbors.predict(x_test)

acc_knn = round(neighbors.score(x_train, y_train) * 100, 2)

# Model Accuracy
print("Train accuracy:",acc_knn)
print("Test Accuracy:", round( 100*(accuracy_score(y_test, y_predKNN)), 2 ))

print("Classification Report : \n", classification_report(y_test, y_predKNN))

# Confusion Matrix
sns.heatmap( data= confusion_matrix(y_test, y_predKNN)  , annot = True, cmap="RdPu",fmt='g')
plt.title('Confusion Matrix', y=1.02)
plt.xlabel('Actual Label')
plt.ylabel('Prediction Label')


results.append(accuracy_score(y_test, y_predKNN))

# Decision Tree

DT = DecisionTreeClassifier()
DT = DT.fit(x_train,y_train)
y_predDT = DT.predict(x_test)

acc_decision_tree = round(DT.score(x_train, y_train) * 100, 2)

# Model Accuracy
print("Train accuracy:",acc_decision_tree)
print("Test Accuracy:", 100*(accuracy_score(y_test, y_predDT)))

print("Classification Report : \n", classification_report(y_test, y_predDT))

# Confusion Matrix
sns.heatmap( data= confusion_matrix(y_test, y_predDT)  , annot = True, cmap="RdPu",fmt='g')
plt.title('Confusion Matrix', y=1.02)
plt.xlabel('Actual Label')
plt.ylabel('Prediction Label')

results.append(accuracy_score(y_test, y_predDT))

RF = RandomForestClassifier(n_estimators=100, random_state=100,  criterion='entropy', min_samples_leaf=50)
RF.fit(x_train, y_train)
y_predRF = RF.predict(x_test)

accRF = round(RF.score(x_train, y_train) * 100, 2)

# Model Accuracy
print("Train accuracy:",accRF)
print("Test Accuracy:", 100*(accuracy_score(y_test, y_predRF)))

print("Classification Report : \n", classification_report(y_test, y_predRF))

# Confusion Matrix
sns.heatmap( data= confusion_matrix(y_test, y_predRF)  , annot = True, cmap="RdPu",fmt='g')
plt.title('Confusion Matrix', y=1.02)
plt.xlabel('Actual Label')
plt.ylabel('Prediction Label')

results.append(accuracy_score(y_test, y_predRF))

def AccComparison():
    Results_DF = pd.DataFrame({'Classifiers': list(names), 'Accuracy': list(results)}).sort_values('Accuracy', ascending=True)
    Results_DF.plot(x='Classifiers', y='Accuracy', kind='line')
    plt.suptitle('Accuracy Comparison')
    plt.show()

AccComparison()